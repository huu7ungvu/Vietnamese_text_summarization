{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv4kB8-DR4iG"
      },
      "source": [
        "## đọc dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PFkHdWwb8xzs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/nam3/course/nlp/keras-text-summarization/demo/data/data_train.csv', encoding='utf-8')\n",
        "df_dev = pd.read_csv('/content/drive/MyDrive/nam3/course/nlp/keras-text-summarization/demo/data/data_val.csv', encoding='utf-8')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/nam3/course/nlp/keras-text-summarization/demo/data/test_data.csv', encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "65D52eDa8b9j"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop(['Unnamed: 0', 'file'], axis=1)\n",
        "df_dev = df_dev.drop(['Unnamed: 0', 'file'], axis=1)\n",
        "df_test = df_test.drop(['Unnamed: 0', 'file'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[0:10000]\n",
        "df_dev = df_dev[0:1000]\n",
        "# data_20k = train_df[0:20000]\n",
        "df_test = df_test[0:1000]"
      ],
      "metadata": {
        "id": "DuH5wJoo-G6a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4_GSCIN8MJD",
        "outputId": "e1f61312-9c75-4d3c-969f-9930b177bf4b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "_u8knMd--kfp",
        "outputId": "7b597c66-f0f4-4320-cd37-6ef391a39692"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HMS Queen_Elizabeth thử_nghiệm trên biển hồi n...</td>\n",
              "      <td>Bộ_trưởng Quốc_phòng Anh vạch ra chiến_lược hậ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bộ_trưởng Công_Thương_Trần_Tuấn_Anh vừa kỷ_luậ...</td>\n",
              "      <td>Nhận kỷ_luật sau khi đi lễ trong giờ_hành_chín...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            original                                            summary\n",
              "0  HMS Queen_Elizabeth thử_nghiệm trên biển hồi n...  Bộ_trưởng Quốc_phòng Anh vạch ra chiến_lược hậ...\n",
              "1  Bộ_trưởng Công_Thương_Trần_Tuấn_Anh vừa kỷ_luậ...  Nhận kỷ_luật sau khi đi lễ trong giờ_hành_chín..."
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df_test.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtfFOCVT8b9l",
        "outputId": "7db55129-f496-4875-b3cf-a41c61bfe346"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Kẻ tấn_công bị cảnh_sát khống_chế . Ảnh : Twit...\n",
              "1    Gia_tăng áp_lực lên Bình_Nhưỡng ? Theo Sputnik...\n",
              "2    Thông_tin từ hãng thông_tấn Nhà_nước Syria SAN...\n",
              "3    Sáng nay 26-1 , đoàn cổ_động_viên này đã làm t...\n",
              "4    Ngày 23/2 , hãng tin Reuters dẫn lời Ngoại_trư...\n",
              "5    Trinh_Phan , 33 tuổi , cùng chồng Young_Nguyen...\n",
              "6    Tiềm_thức người Việt trước_đây luôn nghĩ \" trâ...\n",
              "7    Theo Phó_giáo_sư Hà_Đình_Đức , rùa hồ Gươm nằm...\n",
              "8    Không đạt kế_hoạch Tuy_nhiên , kết_luận thanh_...\n",
              "9    Trong văn_bản tham_mưu gửi UBND TP HCM , Sở Qu...\n",
              "Name: original, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "df_train['original'][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyE_ds8apaFE"
      },
      "source": [
        "## pretrained (W2V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LBW_3GAO8b9v"
      },
      "outputs": [],
      "source": [
        "#Add sostok and eostok at \n",
        "# post_pre_train = df_train\n",
        "# post_pre_dev = df_dev\n",
        "# post_pre_test = df_test\n",
        "df_train['summary'] = df_train['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
        "df_dev['summary'] = df_dev['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
        "df_test['summary'] = df_test['summary'].apply(lambda x : 'sostok '+ x + ' eosto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79TKWhrMW0BG",
        "outputId": "f3fbe498-2724-4b9b-fa4f-ff3391c7ada3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.3.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.1.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.62.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvlxke8pU26f",
        "outputId": "679bf5e9-d06a-4cbc-c1ec-fc0264b10b6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding data loaded\n"
          ]
        }
      ],
      "source": [
        "# Read embedding\n",
        "word_dict = []\n",
        "embeddings_index = {}\n",
        "embedding_dim = 300\n",
        "max_feature = len(embeddings_index) + 2\n",
        "\n",
        "f = open('drive/MyDrive/public_dataset/uit-vsfc/W2V_ner.vec')\n",
        "for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] \n",
        "    word_dict.append(word)\n",
        "    try:\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    except Exception as e:\n",
        "        pass\n",
        "f.close()\n",
        "\n",
        "print('Embedding data loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DSZzC5Gk7nzY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from pyvi import ViTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_len = 100\n",
        "\n",
        "word_tokenizer = Tokenizer(oov_token=-1)\n",
        "word_tokenizer.fit_on_texts(df_train.original.values.flatten())\n",
        "word_to_index = word_tokenizer.word_index\n",
        "word_to_index['pad'] = 0\n",
        "word_to_index['unk'] = -1\n",
        "\n",
        "index_to_word = {i: w for w, i in word_to_index.items()}\n",
        "\n",
        "def encodingx(X):\n",
        "    sentences = []\n",
        "    \n",
        "    for t in X:\n",
        "        tokenized_sentence = ViTokenizer.tokenize(t)\n",
        "        sentences.append(tokenized_sentence)\n",
        "    \n",
        "    X = word_tokenizer.texts_to_sequences(sentences)\n",
        "    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index['pad'])\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7-wnyOA_p3YK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from pyvi import ViTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_len = 100\n",
        "\n",
        "yword_tokenizer = Tokenizer(oov_token=-1)\n",
        "yword_tokenizer.fit_on_texts(df_train.summary.values.flatten())\n",
        "yword_to_index = yword_tokenizer.word_index\n",
        "yword_to_index['pad'] = 0\n",
        "yword_to_index['unk'] = -1\n",
        "\n",
        "yindex_to_word = {i: w for w, i in yword_to_index.items()}\n",
        "\n",
        "def encodingy(X):\n",
        "    sentences = []\n",
        "    \n",
        "    for t in X:\n",
        "        tokenized_sentence = ViTokenizer.tokenize(t)\n",
        "        sentences.append(tokenized_sentence)\n",
        "    \n",
        "    X = yword_tokenizer.texts_to_sequences(sentences)\n",
        "    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = yword_to_index['pad'])\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTW2RFvCYTwW"
      },
      "outputs": [],
      "source": [
        "# X_train_encoded = encoding(X_train)\n",
        "# X_dev_encoded = encoding(X_dev)\n",
        "# X_test_encoded = encoding(X_test)\n",
        "\n",
        "X_train_encoded = encodingx(df_train.original.values.flatten())\n",
        "y_train_encoded = encodingy(df_train.summary)\n",
        "X_dev_encoded = encodingx(df_dev.original.values.flatten())\n",
        "y_dev_encoded = encodingy(df_dev.summary)\n",
        "X_test_encoded = encodingx(df_test.original.values.flatten())\n",
        "y_test_encoded = encodingy(df_test.summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_encoded"
      ],
      "metadata": {
        "id": "toi7ALRalNqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding Custom Attention layer \n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "metadata": {
        "id": "_yJ_1AcP7T-x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiYTnHAyUZ6R",
        "outputId": "eaf66740-3ead-4b2f-c8ad-d35784d5bd5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary from the w2v model = 102692\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     10269200    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 100, 300),   481200      ['embedding[0][0]']              \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 100, 300),   721200      ['lstm[0][0]']                   \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 100)    2315200     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 100, 300),   721200      ['lstm_1[0][0]']                 \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 300),  481200      ['embedding_1[0][0]',            \n",
            "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, None, 23152)  6968752    ['lstm_3[0][0]']                 \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,957,952\n",
            "Trainable params: 21,957,952\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import backend as K \n",
        "import gensim\n",
        "from numpy import *\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.initializers import Constant\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "num_wordsX = len(word_to_index)\n",
        "num_wordsy = len(yword_to_index)\n",
        "\n",
        "print(\"Size of vocabulary from the w2v model = {}\".format(num_wordsX))\n",
        "MAX_LEN=100\n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(MAX_LEN, ))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(input_dim=num_wordsX, \n",
        "                     output_dim=embedding_dim,\n",
        "                    #  embeddings_initializer=Constant(embedding_matrixX),\n",
        "                     trainable=True,\n",
        "                     input_length = MAX_LEN\n",
        "                     \n",
        "                     )(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(input_dim=num_wordsy,\n",
        "                          output_dim=embedding_dim,\n",
        "                          trainable=True,\n",
        "                          # embeddings_initializer=Constant(embedding_matrixy),\n",
        "                          input_length=MAX_LEN)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "#Attention layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "#Concating Attention input and Decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(num_wordsy, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiishsDQxvRP",
        "outputId": "84363ac0-8036-4ac0-938a-526c60de3b5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102692, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "embedding_matrixX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jeic2WhUa90",
        "outputId": "19be2788-8c68-4d90-bac8-88bd1b85e919"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105418, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "X_train_encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr5-groV8b9z"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWFsjN3c8b9z"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixnC2J6QXyzm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.experimental.set_policy(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZj2w8Tn8b9z",
        "outputId": "d5c94bb2-6623-485e-d5de-4c66f4312204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 25/412 [>.............................] - ETA: 14:20 - loss: 4.3796"
          ]
        }
      ],
      "source": [
        "# history=model.fit(x_tr, y_tr,epochs=50,callbacks=[es],batch_size=128, validation_data=(x_val,y_val))\n",
        "\n",
        "history=model.fit([X_train_encoded,y_train_encoded[:,:-1]], y_train_encoded.reshape(y_train_encoded.shape[0],y_train_encoded.shape[1], 1)[:,1:] ,\n",
        "                  epochs=50,callbacks=[es],\n",
        "                  batch_size=256, \n",
        "                  validation_data=([X_dev_encoded,y_dev_encoded[:,:-1]], y_dev_encoded.reshape(y_dev_encoded.shape[0],y_dev_encoded.shape[1], 1)[:,1:]))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkID1NSRMq6W"
      },
      "source": [
        "##decode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "def load_model(model_filename, model_weights_filename):\n",
        "    with open(model_filename, 'r', encoding='utf8') as f:\n",
        "        model = model_from_json(f.read(), custom_objects={'AttentionLayer': AttentionLayer})\n",
        "    model.load_weights(model_weights_filename)\n",
        "    return model\n",
        "\n",
        "encoder = load_model('/content/drive/MyDrive/nam3/course/nlp/encoder_model.json', '/content/drive/MyDrive/nam3/course/nlp/encoder_model_full_seq2seq_vitokenizer.h5')\n",
        "decoder = load_model('/content/drive/MyDrive/nam3/course/nlp/decoder_model.json', '/content/drive/MyDrive/nam3/course/nlp/decoder_model_full_seq2seq_vitokenizer.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajAMhL-j7tcs",
        "outputId": "ddc82545-3448-43bd-a580-7b5168c73ba8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DW1TxcA48b90"
      },
      "outputs": [],
      "source": [
        "reverse_target_word_index=yword_tokenizer.index_word\n",
        "reverse_source_word_index=word_tokenizer.index_word\n",
        "target_word_index=yword_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyURMC9z8b90"
      },
      "outputs": [],
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(MAX_LEN,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#Attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "#Adding Dense softmax layer to generate proability distribution over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7XW6fAz8b90"
      },
      "source": [
        "**We are defining a function below which is the implementation of the inference process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "p7JGRJTj8b90"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (100-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mfl2O9I8b92"
      },
      "source": [
        "**Let us define the functions to convert an integer sequence to a word sequence for summary as well as the reviews:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KBJC-UpN8b92"
      },
      "outputs": [],
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+str(reverse_target_word_index[i])+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+str(reverse_source_word_index[i])+' '\n",
        "    return newString"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0zC_zR08b92"
      },
      "source": [
        "**Run the model over the data to see the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqzHORGG8b92",
        "outputId": "c38d5676-d1d8-46fc-8451-e0b15ae38a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: kiến được trang bị hệ thống phóng và cáp hãm đà cho máy bay như lớp nimitz và gerald r ford của mỹ nhưng chi phí quá cao buộc hải quân anh lựa chọn giải pháp stovl và sử dụng tiêm kích f 35 b hms queen elizabeth có khả năng chở tối đa 36 máy bay f 35 b và 4 trực thăng cảnh báo sớm crowsnest nó có thể được bổ sung tới 12 trực thăng chinook hoặc merlin và 8 trực thăng tấn công ah 64 apache hms queen elizabeth thử nghiệm trên biển hồi năm 2018 \n",
            "Original summary: bộ trưởng quốc phòng anh vạch ra chiến lược hậu brexit đề cao phô trương sức mạnh quân sự để bảo vệ lợi ích của london -1 \n",
            "Predicted summary:  một tiêm kích f 16 của nga đã được cho là một binh sĩ ở syria đã được tìm cách cho các cuộc tấn công bằng quân đội mỹ\n",
            "\n",
            "\n",
            "Review: giờ làm việc xét dấu hiệu vi phạm nghiêm trọng kỷ luật lao động ngày 8 2 bộ trưởng công thương trần tuấn anh đã quyết định thành lập hội đồng kỷ luật để xem xét mức độ vi phạm và khuyến nghị hình thức kỷ luật đối với ông bùi quang hưng đồng thời giao cục xúc tiến thương mại tiến hành họp xem xét hình thức kỷ luật với các cán bộ viên chức còn lại nhiều người dân tìm đến đền chùa đi lễ đầu năm để cầu bình an may mắn ảnh minh hoạ võ hải \n",
            "Original summary: nhận kỷ luật sau khi đi lễ trong giờ hành chính giám đốc trung tâm hỗ trợ xuất khẩu cục xúc tiến thương mại bị hạ bậc lương hội đồng kỷ luật cũng cảnh cáo 7 cán bộ cấp phòng khiển trách 2 viên chức -1 \n",
            "Predicted summary:  bộ trưởng bộ trưởng bộ gd đt tỉnh đắk lắk đã yêu cầu ubnd tỉnh này đã có quyết định khởi tố vụ án khởi tố vụ án của bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng bộ trưởng\n",
            "\n",
            "\n",
            "Review: tại toà án chứ không phải thẩm quyền của sở kh đt hà nội được biết thành viên công ty kim anh đã có đơn khởi kiện các tranh chấp của công ty ra tand tp hà nội sau khi có quyết định thụ lý vụ án hiện tand tp hà nội đã có văn bản yêu cầu sở kh đt hà nội cung cấp các tài liệu liên quan đến vụ án nếu các bên các cơ quan thượng tôn và làm theo pháp luật thì nên chờ đợi phán quyết từ phía toà án còn nữa xuân hoà \n",
            "Original summary: theo quy định của luật khiếu nại các cơ quan nhà nước có thẩm quyền không được thụ lý giải quyết đối với khiếu nại đã có quyết định giải quyết lần 2 việc này thuộc thẩm quyền của toà án -1 \n",
            "Predicted summary:  theo cục trưởng bộ giao thông vận tải vừa bị khởi tố vụ án khởi tố vụ án khởi tố vụ án khởi tố vụ án khởi tố vụ án khởi tố vụ án luật sư phạm pháp luật\n",
            "\n",
            "\n",
            "Review: phố sách vẫn là điểm nhấn đẹp ở thủ đô trong ngày đầu xuân mới và giảm đến 35 trong đó có nhiều bộ sách kinh điển như chiến tranh và hoà bình của -1 -1 ảnh vĩnh hà phụ huynh đưa con đến quầy sách thiếu nhi ảnh vĩnh hà những đứa trẻ trên phố sách ảnh vĩnh hà một thầy đồ đang viết chữ theo đề nghị của người dạo chơi trong phố sách ảnh vĩnh hà tại quầy bán báo xuân trong phố sách ảnh vĩnh hà những người trẻ tuổi đến phố sách ảnh vĩnh hà \n",
            "Original summary: ngày mùng 3 tết phố sách hà nội mở cửa và sẽ kéo dài tới ngày mùng 10 tế và đây trở thành một điểm đến của nhiều gia đình đi du xuân -1 \n",
            "Predicted summary:  sau khi các nhóm khoa học của ông nguyễn văn cương đã bày tỏ ra câu hỏi về việc phân tích trong đó có thể là một số ngôn ngữ đáng chú ý\n",
            "\n",
            "\n",
            "Review: xế taxi cũng vô tình không biết trên xe còn một hành khách anh đậu xe tại một garage ngầm rồi đi ăn trưa để lại em bé sơ sinh đang ngủ trên xe phải tới khi dừng xe mua vé vào một sân bay tài xế mới phát hiện hành khách nhí bị bỏ quên anh mau chóng gọi điện cho cảnh sát và xe cấp cứu đã được điều khẩn cấp tới tiếp nhận em bé thật may mắn khi đứa trẻ sơ sinh vẫn an toàn và được đoàn tụ cùng cha mẹ ảnh minh hoạ -1 \n",
            "Original summary: một cặp vợ chồng tại thành phố hamburg đức đã lơ -1 quên mất đứa con vừa chào đời của họ khi đi taxi từ bệnh viện về nhà -1 \n",
            "Predicted summary:  sáng nay 2 thông tin với pv báo người đưa tin vụ tai nạn giao thông thông tin về vụ tai nạn nghiêm trọng xảy ra tại trạm thu phí bot cai lậy giao thông thông tin về việc kiểm tra hành vi phạm pháp\n",
            "\n",
            "\n",
            "Review: phép riêng về các kiến nghị sửa đổi bổ sung nghị định 33 năm 2017 về quy định xử phạt vi phạm hành chính trong lĩnh vực tài nguyên nước và khoáng sản theo hướng tăng nặng hình thức xử lý ông nhân yêu cầu ubnd tp xây dựng dự thảo những điều cần sửa đổi bổ sung trên cơ sở tham khảo ý kiến các tỉnh thành giáp ranh để sớm trình các cấp thẩm quyền phương tiện khai thác cát trái phép ở cần giờ bị bộ đội biên phòng tp hcm tạm giữ ảnh nguyễn đức thắng \n",
            "Original summary: tại hội nghị thông qua đề án phòng chống khai thác cát trái phép trên vùng biển cần giờ vùng giáp ranh giữa tp hcm và các tỉnh lân cận sáng 23 4 bí thư thành uỷ tp hcm nguyễn thiện nhân đồng ý phương án trên -1 \n",
            "Predicted summary:  theo thông tin của tổng cục thống kê bộ thống nhất bộ trưởng bộ giao thông vận tải và thông tin về việc quản lý dự án luật pháp luật với chủ tịch ubnd tp hcm\n",
            "\n",
            "\n",
            "Review: cầu lê hồng phong khoảng 4h ngày 23 6 hàng chục cảnh sát bộ đội biên phòng được huy động tham gia cứu hộ cầu lê hồng phong được thiết kế đúc bêtông nhưng có dây văng dài hơn 100 m rộng gần 7 m tải trọng 10 tấn khánh thành năm 2002 đây là cầu được đầu tư lớn nhất trong số 3 cầu bắc qua sông cà ty ở nội đô tp phan thiết xem video cảnh sát phá tàu bị kẹt để cứu cầu tư huynh tàu cá kẹt 12 giờ dưới gầm cầu lê hồng phong \n",
            "Original summary: chiều 23 6 cảnh sát cứu hộ tỉnh bình thuận đã kéo tàu cá bị kẹt thoát ra khỏi gầm cầu lê hồng phong cứu nguy cho cầu lớn nhất tp phan thiết -1 \n",
            "Predicted summary:  ngày 28 5 cục trưởng cục giao thông vận tải tp hcm đã thông báo đã đưa ra tại buổi họp báo thông tin về việc xảy ra tại huyện trấn hà tĩnh\n",
            "\n",
            "\n",
            "Review: biệt là những di sản quý hiếm và đang bị lâm nguy được unesco phát động từ năm 1992 trong 16 bộ hồ sơ của 10 nước đệ trình đăng ký công nhận là di sản tư liệu thuộc chương trình ký ức thế giới khu vực châu á thái bình dương năm 201 có 14 bộ hồ sơ được công nhận cả 2 hồ sơ của việt nam được đánh giá cao và đều được công nhận trong dịp này đắc đức những bài thơ văn được chọn lọc được khắc chạm ở điện thái hoà đại nội huế \n",
            "Original summary: thơ văn trên kiến trúc cung đình huế cùng mộc bản trường học phúc giang hà tĩnh được công nhận là di sản tư liệu thế giới thuộc chương trình ký ức khu vực châu á – thái bình dương -1 \n",
            "Predicted summary:  một số những người dân ở biển đã được công ty cổ phần ở hà nội đã được cho là những người dân có thể có những người dân đang được xem xét xử lý\n",
            "\n",
            "\n",
            "Review: đi máy bay có biểu hiện gia tăng gần đây 6 tháng đầu năm 2015 có hơn 160 vụ khiếu nại liên quan hành lý mất cắp trong đó 79 vụ tại sân bay nội bài 88 vụ tại tân sơn nhất công ty cổ phần phục vụ mặt đất hà nội hgs hàng ngày cung cấp dịch vụ mặt đất cho các chuyển bay của hơn 10 hãng hàng không quốc tế và hãng -1 air của việt nam đoàn loan khu vực hầm hàng máy bay đã xảy ra vụ trộm điện thoại ảnh minh -1 đoàn loan \n",
            "Original summary: trong khi bốc dỡ hàng -1 từ hầm hàng xuống máy bay hai nhân viên của đơn vị dịch vụ mặt đất -1 đã lấy trộm điện thoại samsung note trong hành lý của vị khách hàn quốc ngày 20 7 -1 \n",
            "Predicted summary:  các hãng hàng không có chuyến bay chở hàng không khí boeing 737 max đã có khả năng bay không khí thải từ cảng hàng không có chuyến bay không khí bay\n",
            "\n",
            "\n",
            "Review: khiến tử vong hiện trường vụ tai nạn tại hiện trường thi thể ông cương nằm kẹt dưới thùng xe tải bên cạnh là chiếc xe máy bị biến dạng dầu chảy loang lổ chiếc xe tải bị vỡ kính trước tài xế bị thương nhẹ người dân cùng lực lượng chức năng đã dùng phương tiện cứu hộ đưa thi thể ông cương ra ngoài đồng thời nâng chiếc xe tải lên công an huyện can lộc đã có mặt tại hiện phân luồng giao thông điều tra nguyên nhân tai nạn đức hùng hiện trường vụ tai nạn \n",
            "Original summary: không làm chủ được tình huống khi ông cương rẽ qua đường tài xế xe tải bẻ lái sang phải khiến ôtô lật nghiêng đè chết cụ ông 71 tuổi -1 \n",
            "Predicted summary:  chiếc xe container chở xe tải chở khách sạn chỗ bất ngờ lao vào xe máy khiến một người phụ nữ bị thương nặng\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "for i in range(0,10):\n",
        "    print(\"Review:\",seq2text(X_test_encoded[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_test_encoded[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(X_test_encoded[i].reshape(1,100)))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An_KHkDkvau8"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/nam3/course/nlp/model_10k_seq2seq_vitokenizer.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmLOpZuQvnyS"
      },
      "outputs": [],
      "source": [
        "!pip install rouge-score -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVfoUtoSwH0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5dee97c-4df4-43b3-e4c0-68b582fc77da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: kiến được trang bị hệ thống phóng và cáp hãm đà cho máy bay như lớp nimitz và gerald r ford của mỹ nhưng chi phí quá cao buộc hải quân anh lựa chọn giải pháp stovl và sử dụng tiêm kích f 35 b hms queen elizabeth có khả năng chở tối đa 36 máy bay f 35 b và 4 trực thăng cảnh báo sớm crowsnest nó có thể được bổ sung tới 12 trực thăng chinook hoặc merlin và 8 trực thăng tấn công ah 64 apache hms queen elizabeth thử nghiệm trên biển hồi năm 2018 \n",
            "Original summary: bộ trưởng quốc phòng anh vạch ra chiến lược hậu brexit đề cao phô trương sức mạnh quân sự để bảo vệ lợi ích của london -1 \n",
            "Predicted summary:  một máy bay chở bom ở miền nam đã được đưa ra vào các cuộc chiến dịch của mỹ và không có dấu hiệu lực lượng nga\n",
            "{'rouge1': Score(precision=0.5128205128205128, recall=0.5128205128205128, fmeasure=0.5128205128205128), 'rouge2': Score(precision=0.10526315789473684, recall=0.10526315789473684, fmeasure=0.10526315789473684), 'rougeL': Score(precision=0.23076923076923078, recall=0.23076923076923078, fmeasure=0.23076923076923078)}\n",
            "\n",
            "\n",
            "Review: giờ làm việc xét dấu hiệu vi phạm nghiêm trọng kỷ luật lao động ngày 8 2 bộ trưởng công thương trần tuấn anh đã quyết định thành lập hội đồng kỷ luật để xem xét mức độ vi phạm và khuyến nghị hình thức kỷ luật đối với ông bùi quang hưng đồng thời giao cục xúc tiến thương mại tiến hành họp xem xét hình thức kỷ luật với các cán bộ viên chức còn lại nhiều người dân tìm đến đền chùa đi lễ đầu năm để cầu bình an may mắn ảnh minh hoạ võ hải \n",
            "Original summary: nhận kỷ luật sau khi đi lễ trong giờ hành chính giám đốc trung tâm hỗ trợ xuất khẩu cục xúc tiến thương mại bị hạ bậc lương hội đồng kỷ luật cũng cảnh cáo 7 cán bộ cấp phòng khiển trách 2 viên chức -1 \n",
            "Predicted summary:  ông nguyễn thị kim thị kim thị trường thpt phó chủ tịch ubnd tỉnh quảng ninh vừa bị bắt giữ tại trạm bot cai lậy\n",
            "{'rouge1': Score(precision=0.6, recall=0.2876712328767123, fmeasure=0.3888888888888889), 'rouge2': Score(precision=0.029411764705882353, recall=0.013888888888888888, fmeasure=0.018867924528301886), 'rougeL': Score(precision=0.2571428571428571, recall=0.1232876712328767, fmeasure=0.16666666666666663)}\n",
            "\n",
            "\n",
            "Review: tại toà án chứ không phải thẩm quyền của sở kh đt hà nội được biết thành viên công ty kim anh đã có đơn khởi kiện các tranh chấp của công ty ra tand tp hà nội sau khi có quyết định thụ lý vụ án hiện tand tp hà nội đã có văn bản yêu cầu sở kh đt hà nội cung cấp các tài liệu liên quan đến vụ án nếu các bên các cơ quan thượng tôn và làm theo pháp luật thì nên chờ đợi phán quyết từ phía toà án còn nữa xuân hoà \n",
            "Original summary: theo quy định của luật khiếu nại các cơ quan nhà nước có thẩm quyền không được thụ lý giải quyết đối với khiếu nại đã có quyết định giải quyết lần 2 việc này thuộc thẩm quyền của toà án -1 \n",
            "Predicted summary:  công an tỉnh đắk nông vừa bắt giữ vì can bắt tạm giam đối với ông nguyễn văn tuấn phó chủ tịch ubnd tỉnh quảng ninh vừa bị bắt giữ\n",
            "{'rouge1': Score(precision=0.43478260869565216, recall=0.3076923076923077, fmeasure=0.36036036036036034), 'rouge2': Score(precision=0.06666666666666667, recall=0.046875, fmeasure=0.05504587155963303), 'rougeL': Score(precision=0.30434782608695654, recall=0.2153846153846154, fmeasure=0.2522522522522523)}\n",
            "\n",
            "\n",
            "Review: phố sách vẫn là điểm nhấn đẹp ở thủ đô trong ngày đầu xuân mới và giảm đến 35 trong đó có nhiều bộ sách kinh điển như chiến tranh và hoà bình của -1 -1 ảnh vĩnh hà phụ huynh đưa con đến quầy sách thiếu nhi ảnh vĩnh hà những đứa trẻ trên phố sách ảnh vĩnh hà một thầy đồ đang viết chữ theo đề nghị của người dạo chơi trong phố sách ảnh vĩnh hà tại quầy bán báo xuân trong phố sách ảnh vĩnh hà những người trẻ tuổi đến phố sách ảnh vĩnh hà \n",
            "Original summary: ngày mùng 3 tết phố sách hà nội mở cửa và sẽ kéo dài tới ngày mùng 10 tế và đây trở thành một điểm đến của nhiều gia đình đi du xuân -1 \n",
            "Predicted summary:  những học sinh lớp 12 trường thpt quốc gia 2019 đã được công bố được giải thưởng của các trường thi thpt quốc gia 2019\n",
            "{'rouge1': Score(precision=0.3611111111111111, recall=0.2549019607843137, fmeasure=0.29885057471264365), 'rouge2': Score(precision=0.02857142857142857, recall=0.02, fmeasure=0.023529411764705885), 'rougeL': Score(precision=0.25, recall=0.17647058823529413, fmeasure=0.20689655172413793)}\n",
            "\n",
            "\n",
            "Review: xế taxi cũng vô tình không biết trên xe còn một hành khách anh đậu xe tại một garage ngầm rồi đi ăn trưa để lại em bé sơ sinh đang ngủ trên xe phải tới khi dừng xe mua vé vào một sân bay tài xế mới phát hiện hành khách nhí bị bỏ quên anh mau chóng gọi điện cho cảnh sát và xe cấp cứu đã được điều khẩn cấp tới tiếp nhận em bé thật may mắn khi đứa trẻ sơ sinh vẫn an toàn và được đoàn tụ cùng cha mẹ ảnh minh hoạ -1 \n",
            "Original summary: một cặp vợ chồng tại thành phố hamburg đức đã lơ -1 quên mất đứa con vừa chào đời của họ khi đi taxi từ bệnh viện về nhà -1 \n",
            "Predicted summary:  sau khi xảy ra mâu thuẫn với người dân ở huyện quỳnh phúc đã được đưa tin về việc làm việc làm việc\n",
            "{'rouge1': Score(precision=0.5428571428571428, recall=0.4634146341463415, fmeasure=0.5), 'rouge2': Score(precision=0.11764705882352941, recall=0.1, fmeasure=0.1081081081081081), 'rougeL': Score(precision=0.3142857142857143, recall=0.2682926829268293, fmeasure=0.2894736842105263)}\n",
            "\n",
            "\n",
            "Review: phép riêng về các kiến nghị sửa đổi bổ sung nghị định 33 năm 2017 về quy định xử phạt vi phạm hành chính trong lĩnh vực tài nguyên nước và khoáng sản theo hướng tăng nặng hình thức xử lý ông nhân yêu cầu ubnd tp xây dựng dự thảo những điều cần sửa đổi bổ sung trên cơ sở tham khảo ý kiến các tỉnh thành giáp ranh để sớm trình các cấp thẩm quyền phương tiện khai thác cát trái phép ở cần giờ bị bộ đội biên phòng tp hcm tạm giữ ảnh nguyễn đức thắng \n",
            "Original summary: tại hội nghị thông qua đề án phòng chống khai thác cát trái phép trên vùng biển cần giờ vùng giáp ranh giữa tp hcm và các tỉnh lân cận sáng 23 4 bí thư thành uỷ tp hcm nguyễn thiện nhân đồng ý phương án trên -1 \n",
            "Predicted summary:  ông nguyễn văn minh phó chủ tịch ubnd tp hcm vừa có quyết định khởi tố vụ án khởi tố vụ án khởi tố vụ án khởi tố vụ án khởi tố vụ án\n",
            "{'rouge1': Score(precision=0.5681818181818182, recall=0.3424657534246575, fmeasure=0.4273504273504274), 'rouge2': Score(precision=0.11627906976744186, recall=0.06944444444444445, fmeasure=0.08695652173913043), 'rougeL': Score(precision=0.36363636363636365, recall=0.2191780821917808, fmeasure=0.2735042735042735)}\n",
            "\n",
            "\n",
            "Review: cầu lê hồng phong khoảng 4h ngày 23 6 hàng chục cảnh sát bộ đội biên phòng được huy động tham gia cứu hộ cầu lê hồng phong được thiết kế đúc bêtông nhưng có dây văng dài hơn 100 m rộng gần 7 m tải trọng 10 tấn khánh thành năm 2002 đây là cầu được đầu tư lớn nhất trong số 3 cầu bắc qua sông cà ty ở nội đô tp phan thiết xem video cảnh sát phá tàu bị kẹt để cứu cầu tư huynh tàu cá kẹt 12 giờ dưới gầm cầu lê hồng phong \n",
            "Original summary: chiều 23 6 cảnh sát cứu hộ tỉnh bình thuận đã kéo tàu cá bị kẹt thoát ra khỏi gầm cầu lê hồng phong cứu nguy cho cầu lớn nhất tp phan thiết -1 \n",
            "Predicted summary:  sáng nay 10 7 tại tp hcm đã xảy ra vụ án xe tải chở xe tải chở xe tải bị thương nặng\n",
            "{'rouge1': Score(precision=0.4, recall=0.22641509433962265, fmeasure=0.2891566265060241), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.3, recall=0.16981132075471697, fmeasure=0.21686746987951805)}\n",
            "\n",
            "\n",
            "Review: biệt là những di sản quý hiếm và đang bị lâm nguy được unesco phát động từ năm 1992 trong 16 bộ hồ sơ của 10 nước đệ trình đăng ký công nhận là di sản tư liệu thuộc chương trình ký ức thế giới khu vực châu á thái bình dương năm 201 có 14 bộ hồ sơ được công nhận cả 2 hồ sơ của việt nam được đánh giá cao và đều được công nhận trong dịp này đắc đức những bài thơ văn được chọn lọc được khắc chạm ở điện thái hoà đại nội huế \n",
            "Original summary: thơ văn trên kiến trúc cung đình huế cùng mộc bản trường học phúc giang hà tĩnh được công nhận là di sản tư liệu thế giới thuộc chương trình ký ức khu vực châu á – thái bình dương -1 \n",
            "Predicted summary:  ngày 20 11 tại tp hcm đã có văn bản gửi báo cáo về việc làm việc với các doanh nghiệp và công bố của công ty tnhh mtv việt nam\n",
            "{'rouge1': Score(precision=0.5106382978723404, recall=0.38095238095238093, fmeasure=0.43636363636363634), 'rouge2': Score(precision=0.15217391304347827, recall=0.11290322580645161, fmeasure=0.12962962962962962), 'rougeL': Score(precision=0.2978723404255319, recall=0.2222222222222222, fmeasure=0.2545454545454546)}\n",
            "\n",
            "\n",
            "Review: đi máy bay có biểu hiện gia tăng gần đây 6 tháng đầu năm 2015 có hơn 160 vụ khiếu nại liên quan hành lý mất cắp trong đó 79 vụ tại sân bay nội bài 88 vụ tại tân sơn nhất công ty cổ phần phục vụ mặt đất hà nội hgs hàng ngày cung cấp dịch vụ mặt đất cho các chuyển bay của hơn 10 hãng hàng không quốc tế và hãng -1 air của việt nam đoàn loan khu vực hầm hàng máy bay đã xảy ra vụ trộm điện thoại ảnh minh -1 đoàn loan \n",
            "Original summary: trong khi bốc dỡ hàng -1 từ hầm hàng xuống máy bay hai nhân viên của đơn vị dịch vụ mặt đất -1 đã lấy trộm điện thoại samsung note trong hành lý của vị khách hàn quốc ngày 20 7 -1 \n",
            "Predicted summary:  công an tỉnh thanh hoá vừa bắt giữ vụ án giết người dân bị bắt giữ tại khu vực cư dân huyện nam sơn tỉnh quảng nam\n",
            "{'rouge1': Score(precision=0.6190476190476191, recall=0.41935483870967744, fmeasure=0.5), 'rouge2': Score(precision=0.04878048780487805, recall=0.03278688524590164, fmeasure=0.03921568627450981), 'rougeL': Score(precision=0.3333333333333333, recall=0.22580645161290322, fmeasure=0.2692307692307692)}\n",
            "\n",
            "\n",
            "Review: khiến tử vong hiện trường vụ tai nạn tại hiện trường thi thể ông cương nằm kẹt dưới thùng xe tải bên cạnh là chiếc xe máy bị biến dạng dầu chảy loang lổ chiếc xe tải bị vỡ kính trước tài xế bị thương nhẹ người dân cùng lực lượng chức năng đã dùng phương tiện cứu hộ đưa thi thể ông cương ra ngoài đồng thời nâng chiếc xe tải lên công an huyện can lộc đã có mặt tại hiện phân luồng giao thông điều tra nguyên nhân tai nạn đức hùng hiện trường vụ tai nạn \n",
            "Original summary: không làm chủ được tình huống khi ông cương rẽ qua đường tài xế xe tải bẻ lái sang phải khiến ôtô lật nghiêng đè chết cụ ông 71 tuổi -1 \n",
            "Predicted summary:  xe khách chở xe tải chở xe tải lao vào xe máy khiến người dân hoảng loạn khiến người dân hoảng loạn\n",
            "{'rouge1': Score(precision=0.4864864864864865, recall=0.4090909090909091, fmeasure=0.44444444444444453), 'rouge2': Score(precision=0.1111111111111111, recall=0.09302325581395349, fmeasure=0.10126582278481011), 'rougeL': Score(precision=0.32432432432432434, recall=0.2727272727272727, fmeasure=0.2962962962962963)}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
        "for i in range(0,10):\n",
        "    print(\"Review:\",seq2text(X_test_encoded[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_test_encoded[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(X_test_encoded[i].reshape(1,100)))\n",
        "    scores = scorer.score(seq2summary(y_test_encoded[i]), decode_sequence(X_test_encoded[i].reshape(1,100)))\n",
        "    print(scores)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3LjqyjgDl4HN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "seq2seq_fineturn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}